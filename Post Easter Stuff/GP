# -*- coding: utf-8 -*-
"""
Created on Thu Mar  8 12:59:01 2018

@author: mtech
"""

import numpy as np
import matplotlib.pyplot as plt
import scipy.optimize as op
import george
import emcee # use for error GP optimization
from george import kernels

#Get data from files 
#tav pav and gap
#function to read in data 
def readfile(filename):
    #open file
    filedata = open(filename, 'r')
    #create blank arrays 
    tAv, pAv, pUncertAv = ([] for i in range(3))
    #while not at the end...
    while True:
        line = filedata.readline() #read the lines
        if not line: break #end infinite loop if no more lines
                                   
        items = line.split(',') #split the items in each line by ','
        
        tAv.append(float(items[0]))
        pAv.append(float(items[1]))
        pUncertAv.append(float(items[2]))
        
    return tAv, pAv, pUncertAv

#********************************************************************************************
#Main
#PARAMETERS TO CHANGE 
baselineNum = 0
sampleNum = 50 # Number of points to calculate in reduced dataset (only used for plot)
sampleNumFull = 5000 # Number of points to calculate for full dataset
numDataPoints = 190 # 4736 is full dataset


#read in data from selected baseline
tAvFull, pAvFull, pUncertAvFull = readfile("{}datafile.txt".format(baselineNum))

#TEST STUFF - REDUCE SIZE OF DATASET FOR USE IN OPTIMIZAION
tAv = tAvFull[0:numDataPoints]
pAv = pAvFull[0:numDataPoints]
pUncertAv = pUncertAvFull[0:numDataPoints]

#Now fit this data using a Guassian model
#CALCULATE COVARIANCE MATRIX - Use relevent kernals

# Squared exponential kernel, takes into account long term rise
k1 = 0.1**2 * kernels.RationalQuadraticKernel(log_alpha = (0.0)**2, metric=(23280)**2)
#long term periodicity
k2 = 0.5**2 * kernels.ExpSquaredKernel(3000**2) * kernels.ExpSine2Kernel(gamma=2**2, log_period=0.0)
# rational quadratic kernel for short term irregularities.
k3 = 0.7**2 * kernels.RationalQuadraticKernel(log_alpha = (0.0)**2, metric=(1)**2)
#short term periodicity
k4 = 0.3**2 * kernels.ExpSquaredKernel(300**2) * kernels.ExpSine2Kernel(gamma=2**2, log_period=0.0)
#combine kernels
kernel = k1 + k2 + k3 + k4

#initiates combines kernels in george library 
gp = george.GP(kernel, mean=np.mean(pAv), fit_mean=True)

#compues covarience matrix - (wants an array, not a list)
gp.compute(tAv, pUncertAv)
print("initial log likelihood before optimization of reduced data set: ")
print (gp.log_likelihood(pAv))

"""
# -------- WITHOUT ERROR OPTIMIZATION AND PLOT --------- #
#NOW OPTIMISING PARAMETERS 
#theres an optimising function in the scipy library
#need to feed in a function to optimise and the gradient of the function
#the function to optimise is the log liklihood, which is on the george library 
# Define the objective function (negative log-likelihood in this case).
def nll(prob):
    gp.set_parameter_vector(prob)
    ll = gp.log_likelihood(pAv, quiet=True)
    return -ll if np.isfinite(ll) else 1e25
# And the gradient of the objective function.
def grad_nll(prob):
    gp.set_parameter_vector(prob)
    return -gp.grad_log_likelihood(pAv, quiet=True)

# Run the optimization routine.
# Initial parameter estimates
p0 = gp.get_parameter_vector()
# Feed into optimisation routine
results = op.minimize(nll, p0, jac=grad_nll, method="L-BFGS-B")
# Update the kernel and print the final log-likelihood. (results.x are the optimized hyperparams)
gp.set_parameter_vector(results.x)
print(gp.log_likelihood(pAv))

#Predict the values of mean and variance at the chosen point
#Produce an array which contains time stamps where there should be a datapoint
x = np.linspace(min(tAv), max(tAv), sampleNum)

mu, var = gp.predict(pAv, x, return_var=True)
std = np.sqrt(var)

#Plot original data and data and mean/var of predicted data
plt.figure()
plt.scatter(x, mu , marker = ".", label = "Predicted data")
plt.scatter(tAv, pAv, marker = "+", label = "Original data")
plt.fill_between(x, mu+std, mu-std, color="g", alpha=0.5)
plt.legend()
"""

# --------------- WITH ERROR OPTIMIZATION AND PLOT --------------
# TO DO - HOW DOES THIS ALL WORK...?
# A slightly different optimization - WITH experimental error included in model - assuming possible correlation in noise
# Define optimization routine
def lnprob2(p):
    gp.set_parameter_vector(p)
    return gp.log_likelihood(pAv, quiet=True) + gp.log_prior()

initial = gp.get_parameter_vector()
ndim, nwalkers = len(initial), 36
# Use emcee to do optimization, as opposed to scipy optimization as before
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob2)

# print("Running first burn-in...")
p0 = initial + 1e-8 * np.random.randn(nwalkers, ndim)
p0, lp, _ = sampler.run_mcmc(p0, 100)

# print("Running second burn-in...")
p0 = p0[np.argmax(lp)] + 1e-8 * np.random.randn(nwalkers, ndim)
sampler.reset()
p0, _, _ = sampler.run_mcmc(p0, 100)
sampler.reset()

# print("Running production...")
sampler.run_mcmc(p0, 100);

print("middle log likelihood after optimization of reduced data set: ")
print(gp.log_likelihood(pAv))


# -------- WITH ERROR PLOT --------- #
#Produce an array which contains time stamps where there should be a datapoint
x = np.linspace(min(tAv), max(tAv), sampleNum)
# Plot the data.
plt.figure()
plt.errorbar(tAv, pAv, pUncertAv, fmt=".k", capsize=0)
# Plot 24 posterior samples.
samples = sampler.flatchain
for s in samples[np.random.randint(len(samples), size=24)]:
    gp.set_parameter_vector(s)
    mu = gp.sample_conditional(pAv, x)
    plt.plot(x, mu, color="#4682b4", alpha=0.3)
             
"""
# OPTIMIZATION OF NEXT CHUNK OF DATA
# GO again, for the next chunk of data
#compues covarience matrix - wants an array, not a list
gp.compute(tAvFull[numDataPoints:2*numDataPoints], pUncertAvFull[numDataPoints:2*numDataPoints])
initial = gp.get_parameter_vector()
# Use emcee to do optimization, as opposed to scipy optimization as before
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob2)
# print("Running first burn-in...")
p0 = initial + 1e-8 * np.random.randn(nwalkers, ndim)
p0, lp, _ = sampler.run_mcmc(p0, 100)
# print("Running second burn-in...")
p0 = p0[np.argmax(lp)] + 1e-8 * np.random.randn(nwalkers, ndim)
sampler.reset()
p0, _, _ = sampler.run_mcmc(p0, 100)
sampler.reset()
# print("Running production...")
sampler.run_mcmc(p0, 100);

print(" log likelihood for next bloc of data, using first optimization as initial parameters, after optimization of reduced data set: ")
print(gp.log_likelihood(pAvFull[numDataPoints:2*numDataPoints]))

# -------- WITH ERROR PLOT --------- #
#Produce an array which contains time stamps where there should be a datapoint
x = np.linspace(min(tAvFull[numDataPoints:2*numDataPoints]), max(tAvFull[numDataPoints:2*numDataPoints]), sampleNum)
# Plot the data.
plt.errorbar(tAvFull[numDataPoints:2*numDataPoints], pAvFull[numDataPoints:2*numDataPoints], pUncertAvFull[numDataPoints:2*numDataPoints], fmt=".k", capsize=0)
# Plot 24 posterior samples.
samples = sampler.flatchain
for s in samples[np.random.randint(len(samples), size=24)]:
    gp.set_parameter_vector(s)
    mu = gp.sample_conditional(pAvFull[numDataPoints:2*numDataPoints], x)
    plt.plot(x, mu, color="r", alpha=0.3)
"""

# GP OF FULL DATA BASED ON OPTIMIZATION OF INITIAL PARAMETERS
# Given GP run on reduced subset of data - fit the remaining data
#Produce an array which contains time stamps where there should be a datapoint
# on a new figure plot the full GP. Compute GP for full set
gp.compute(tAvFull, pUncertAvFull)
plt.figure()
#Produce an array which contains time stamps where there should be a datapoint
x = np.linspace(min(tAvFull), max(tAvFull), sampleNumFull)
print("Final log likelihood after optimization of full data set: ")
print(gp.log_likelihood(pAvFull))
# Predict points
mu, var = gp.predict(pAvFull, x, return_var=True)
std = np.sqrt(var)
# Plot the data.
plt.errorbar(tAvFull, pAvFull, pUncertAvFull, fmt=".k", capsize=0, label = "Original data")
plt.plot(x, mu)
plt.fill_between(x, mu+std, mu-std, color="grey", alpha=0.5)
plt.legend()
